{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Image-GAN (CS445 Project)\n",
    "\n",
    "In this assignment, we will test three image generative adversarial networks (GAN) against threee sets of images. Each images set has different characteristics, as does each GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To begin, the standard imports, as well as setting a random seed for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import clear_output, display\n",
    "%matplotlib inline\n",
    "\n",
    "manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "print(\"Seed:\", manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Here, we load our three datasets. In addition, we set some global variables and intialize cuda if it is available.\n",
    "\n",
    "The first dataset is images of mountains. This image set is quite diverse, and we expect each GAN to perform poorly in creating images that can stand in place of true photographs.\n",
    "\n",
    "The second dataset is of dogs. While still quite diverse, this image set is less complicated in terms of general structure as we shift from mountain landscapes of all types to stills of similar animals.\n",
    "\n",
    "The third and final dataset is of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit to View and Create More Images\n",
    "\n",
    "batch_size = 128\n",
    "img_shape = (3, 64, 64)\n",
    "\n",
    "# Create the Datasets\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_shape[1]),\n",
    "        transforms.CenterCrop(img_shape[1]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_mountains = dset.ImageFolder(root=\"../data/mountains\", transform=transform)\n",
    "dataset_dogs = dset.ImageFolder(root=\"../data/dogs\", transform=transform)\n",
    "dataset_faces = dset.ImageFolder(root=\"../data/faces\", transform=transform)\n",
    "\n",
    "dataloader_mountains = torch.utils.data.DataLoader(dataset_mountains, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "dataloader_dogs = torch.utils.data.DataLoader(dataset_dogs, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "dataloader_faces = torch.utils.data.DataLoader(dataset_faces, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "print(\"Running On Cuda:\", cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Images\n",
    "\n",
    "Here are example images from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader_mountains))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mountain Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:32], padding=2, normalize=True).cpu(),(1,2,0)));\n",
    "\n",
    "real_batch = next(iter(dataloader_dogs))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Dog Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:32], padding=2, normalize=True).cpu(),(1,2,0)));\n",
    "\n",
    "real_batch = next(iter(dataloader_faces))\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Face Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:32], padding=2, normalize=True).cpu(),(1,2,0)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Global Functions\n",
    "\n",
    "Here we define a few functions that we will use to give structure to this notebook. Each GAN uses a similar setup structure, so we will reuse the functions to showcase the resulting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%capture\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_results(dataloader, img_list):\n",
    "    \n",
    "    real_batch = next(iter(dataloader))\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Real Images\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:32], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(img_list[-1][:32], padding=5, normalize=True),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_image_list(images):\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    for epoch, img_set in enumerate(images, 1):\n",
    "        \n",
    "        plt.imshow(img_set[0].permute([1,2,0]))\n",
    "        plt.title(f\"Epoch {epoch}\")\n",
    "        clear_output(wait=True)\n",
    "        display(plt.gcf())\n",
    "    \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def do_run(dataloader, epochs=200, show_static_results=True, verbose=True):\n",
    "\n",
    "    netG = Generator()\n",
    "    netD = Discriminator()\n",
    "    \n",
    "    if cuda:\n",
    "        \n",
    "        netG.cuda()\n",
    "        netD.cuda()\n",
    "        \n",
    "    img_list = main_loop(dataloader, netG, netD, epochs, verbose)\n",
    "    \n",
    "    if show_static_results:\n",
    "    \n",
    "        show_results(dataloader, img_list)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        animate_image_list(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Traditional GAN\n",
    "\n",
    "To begin with, we attempt to run a traditional GAN on the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            \n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            \n",
    "            if normalize:\n",
    "                \n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "                \n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), * img_shape)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main_loop(dataloader, netG, netD, epochs, verbose):\n",
    "    \n",
    "    adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "    if cuda:\n",
    "        \n",
    "        adversarial_loss.cuda()\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    img_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "            valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "            gen_imgs = netG(z)\n",
    "            g_loss = adversarial_loss(netD(gen_imgs), valid)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            real_loss = adversarial_loss(netD(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(netD(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        if verbose:\n",
    "            \n",
    "            print(\"[Epoch %d/%d] [Loss_D: %.4f] [Loss_G: %.4f]\" % (epoch + 1, epochs, d_loss.item(), g_loss.item()), end='\\r')\n",
    "                                \n",
    "        with torch.no_grad():\n",
    "\n",
    "            fake = netG(z).detach().cpu()\n",
    "                \n",
    "        img_list.append(fake)\n",
    "                            \n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wasserstein GAN\n",
    "\n",
    "The Wasserstein GAN [(paper here)](https://arxiv.org/abs/1701.07875) is a model that improves learning stability and helps get rid of problems like mode collapse, which we experienced with our first GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            \n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            \n",
    "            if normalize:\n",
    "                \n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            \n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], * img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main_loop(dataloader, netG, netD, epochs, verbose):\n",
    "        \n",
    "    optimizer_G = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    img_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "            fake_imgs = netG(z)\n",
    "\n",
    "            real_validity = netD(real_imgs)\n",
    "            fake_validity = netD(fake_imgs)\n",
    "            \n",
    "            gradient_penalty = compute_gradient_penalty(netD, real_imgs.data, fake_imgs.data)\n",
    "\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + 10 * gradient_penalty\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "\n",
    "                fake_imgs = netG(z)\n",
    "\n",
    "                fake_validity = netD(fake_imgs)\n",
    "                g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "        print(\"[Epoch %d/%d] [Loss_D: %.4f] [Loss_G: %.4f]\" % (epoch + 1, epochs, d_loss.item(), g_loss.item()), end='\\r')\n",
    "            \n",
    "        with torch.no_grad():\n",
    "\n",
    "            fake = netG(z).detach().cpu()\n",
    "                \n",
    "        img_list.append(fake)\n",
    "                            \n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Relativistic Average GAN\n",
    "\n",
    "The Relativistic Average GAN [(paper here)](https://arxiv.org/abs/1807.00734) is a significantly more stable model that can generate higher quality data samples than their non-relativistic counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = 64 // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(100, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            \n",
    "            if bn:\n",
    "                \n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "           \n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(3, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        ds_size = 64 // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main_loop(dataloader, netG, netD, epochs, verbose):\n",
    "\n",
    "    adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if cuda:\n",
    "        \n",
    "        adversarial_loss.cuda()\n",
    "        \n",
    "    optimizer_G = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    img_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "            valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "            gen_imgs = netG(z)\n",
    "\n",
    "            real_pred = netD(real_imgs).detach()\n",
    "            fake_pred = netD(gen_imgs)\n",
    "\n",
    "            g_loss = adversarial_loss(netD(gen_imgs), valid)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            real_pred = netD(real_imgs)\n",
    "            fake_pred = netD(gen_imgs.detach())\n",
    "\n",
    "            real_loss = adversarial_loss(real_pred - fake_pred.mean(0, keepdim=True), valid)\n",
    "            fake_loss = adversarial_loss(fake_pred - real_pred.mean(0, keepdim=True), fake)\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        if verbose:\n",
    "            \n",
    "            print(\"[Epoch %d/%d] [Loss_D: %.4f] [Loss_G: %.4f]\" % (epoch + 1, epochs, d_loss.item(), g_loss.item()), end='\\r')\n",
    "                    \n",
    "        with torch.no_grad():\n",
    "\n",
    "            fake = netG(z).detach().cpu()\n",
    "                \n",
    "        img_list.append(fake)\n",
    "                            \n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_mountains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run(dataloader_three)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}